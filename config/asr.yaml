trainer: 
  max_epochs: &max_epoch 200
  accelerator: 'auto'
  devices: 1
  log_every_n_steps: 10
  
task:
  _target_: ezspeech.task.asr.ASR_ctc_task
  model:
    architecture:
      _target_: ezspeech.models.conformer_asr.Conformer
      d_input: 128
      d_dim: &d_hidden 512
      num_heads: 8
      num_layers: 12
      depthwise_conv_kernel_size: 31
      vocab_size: 98

    criterion:
      _target_: ezspeech.losses.ctc.CTCLoss
      reduction: 'mean'
      zero_infinity: True
    manager:
      _target_: ezspeech.modules.optimizer.AdamOptimizer
      optimizer:
        lr: 2e-5
        betas: [ 0.9, 0.999 ]
        eps: 1e-9
        weight_decay: 0.01
      scheduler:
        T_max: *max_epoch
        eta_min: 1e-6

  dataset:
    augmentation:
      time_masking:
        _target_: ezspeech.data.augment.time_masking
        proportion: 0.2
      frequency_masking:
        _target_: ezspeech.data.augment.frequency_masking
        proportion: 0.2

    trainset:
      _target_: ezspeech.data.asr.ASRDataset
      filepath: /home4/khanhnd/vivos/train.tsv
      vocab_file: ezspeech/resources/vocab.txt
      augment_prob: 0.2
    valset:
      _target_: ezspeech.data.asr.ASRDataset
      filepath: /home4/khanhnd/vivos/test.tsv
      vocab_file: ezspeech/resources/vocab.txt
    batch_size: 8

callbacks:
  checkpoint: 
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: val_loss
    filename: 'model-{epoch:02d}-{val_loss:.2f}'
    save_top_k: 3
    save_last: True
    mode: 'min'
    every_n_epochs: 1
  lr_monitor:
    _target_: pytorch_lightning.callbacks.LearningRateMonitor
  swa:
    _target_: pytorch_lightning.callbacks.StochasticWeightAveraging
    swa_epoch_start: 0.8
    swa_lrs: 1e-2
    annealing_epochs: 1

logger:
  _target_: pytorch_lightning.loggers.TensorBoardLogger
  save_dir: log
  name: 'lightning_logs'
  version: test_code
  

  # val_check_interval: 1  #
