task:
  _target_: ezspeech.tasks.recognition.SpeechRecognitionTask

  dataset:
    vocab: &vocab ezspeech/resource/vocab.txt
    train_ds:
      _target_: ezspeech.modules.dataset.dataset.SpeechRecognitionDataset
      filepaths:
        - /home4/khanhnd/Ezspeech/data/mix.jsonl
      vocab: *vocab
      augmentation:
        # audio_augment:
          # speed_perturbation:
          #   _target_: ezspeech.datas.augment.SpeedPerturbation
          #   orig_freqs: 16000
          #   factors: [0.9, 1.1, 1.0, 1.0, 1.0]

          # pitchshift:
          #   _target_: ezspeech.datas.augment.PitchShift
          #   min_step: -5
          #   max_step: 5
          #   sample_rates: [16000]
          #   probability: 0.2

          # rir_noise:
          #   _target_: ezspeech.datas.augment.ApplyImpulseResponse
          #   rir_filepath_16k: /data/sondd9/noise-16k/RIR.json
          #   second_before_peak: 0.01
          #   second_after_peak: 0.5
          #   probability: 0.2

          # background_noise:
          #   _target_: ezspeech.augment.AddBackgroundNoise
          #   noise_filepath_16k: /data/sondd9/noise-16k/background-noise.json
          #   # noise_filepath_8k: /dataset/8k/noise/background-noise.json
          #   min_snr_db: 0.0
          #   max_snr_db: 30.0
          #   probability: 0.2

        feature_augment:
          freq_masking:
            _target_: ezspeech.modules.dataset.augment.FrequencyMasking
            freq_masks: 1
            freq_width: 27

          time_masking:
            _target_: ezspeech.modules.dataset.augment.TimeMasking
            time_masks: 10
            time_width: 0.05


    val_ds:
      _target_: ezspeech.modules.dataset.dataset.SpeechRecognitionDataset
      filepaths:
        - /home4/khanhnd/Ezspeech/data/test.jsonl

      vocab: *vocab
    loaders:
      batch_size: 4
      num_workers: 8
      pin_memory: false

  model:
    d_model: &d_model 512
    vocab_size: &vocab_size 1830
    # pretrained_weights: /data/datbt7/checkpoints/asr-offline-60000h.ckpt


    encoder:
      _target_: ezspeech.modules.encoder.squeezeformer.SqueezeFormerEncoder
      input_dim: 128
      d_model: *d_model
      num_layers: 12
      subsampling_factor: 4
      subsampling_num_filters: 256
      subsampling_kernel_size: 3
      attn_num_heads: 8
      attn_group_size: 3
      attn_max_pos_encoding: 10000
      conv_kernel_size: 31
      dropout: 0.1
    decoder:
      _target_: ezspeech.modules.decoder.decoder.CTCDecoder
      input_dim: *d_model
      hidden_dim: *d_model
      output_dim: *vocab_size

    predictor:
      _target_: ezspeech.modules.decoder.decoder.PredictorNetwork
      num_embeddings: *vocab_size
      embedding_dim: *d_model
      d_model: *d_model
      dropout: 0.1

    joint:
      _target_: ezspeech.modules.decoder.decoder.JointNetwork
      input_dim: *d_model
      output_dim: *vocab_size

    criterion:
      _target_: ezspeech.modules.losses.hybrid.HybridRNNT_CTC
      ctc_loss:
        _target_: ezspeech.modules.losses.ctc.CTCLoss
        blank: 0
        reduction: mean_batch
        zero_infinity: true
      rnnt_loss:
        _target_: ezspeech.modules.losses.rnnt_pytorch.RNNTLossPytorch
        blank: 0
        reduction: mean
      ctc_weight: 1.0
      rnnt_weight: 1.0

    optimizer:
      lr: 1
      betas: [0.9, 0.999]
      weight_decay: 1e-2
      eps: 1e-9
      foreach: False
      fused: True

    scheduler:
      d_model: *d_model
      warmup_steps: 10000

callbacks:
  lr:
    _target_: pytorch_lightning.callbacks.LearningRateMonitor

  swa:
    _target_: pytorch_lightning.callbacks.StochasticWeightAveraging
    swa_lrs: 1e-6
    swa_epoch_start: 0.8
    annealing_epochs: 1

  cb:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: step
    save_top_k: 3
    save_last: True
    filename: "{epoch}-{val_ctc_loss:.5f}"
    every_n_epochs: 1

loggers:
  tb:
    _target_: pytorch_lightning.loggers.TensorBoardLogger
    save_dir: ../lightning_logs/oov
    name: null
    version: testcode

    default_hp_metric: false

trainer:
  max_epochs: 60
  # strategy: ddp
  accelerator: gpu
  devices: [1]
  accumulate_grad_batches: 2
  # detect_anomaly: True
  # precision: 16
  val_check_interval: 0.1