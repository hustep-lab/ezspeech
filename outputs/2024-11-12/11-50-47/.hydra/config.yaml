task:
  _target_: ezspeech.task.asr.ASR_ctc_task
  model:
    architecture:
      _target_: ezspeech.models.conformer.Conformer
      d_input: 128
      d_dim: 512
      num_heads: 4
      ffn_dim: 128
      num_layers: 4
      depthwise_conv_kernel_size: 31
      vocab_size: 98
    criterion:
      _target_: ezspeech.modules.criterion.CTCLoss
      reduction: mean
      zero_infinity: false
    manager:
      _target_: ezspeech.modules.optimizer.AdamOptimizer
      optimizer:
        lr: 0.001
        betas:
        - 0.9
        - 0.999
        eps: 1.0e-09
        weight_decay: 0.01
      scheduler:
        mode: min
        factor: 0.1
        patience: 10
        threshold: 0.0001
        threshold_mode: rel
  dataset:
    trainset:
      _target_: ezspeech.data.asr.ASRDataset
      filepath: train.tsv
      vocab_file: ezspeech/resources/vocab.txt
      feature_type: Mel
    valset:
      _target_: ezspeech.data.asr.ASRDataset
      filepath: train.tsv
      vocab_file: ezspeech/resources/vocab.txt
      feature_type: Mel
    batch_size: 2
callbacks:
  checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: val_loss
    filename: model-{epoch:02d}-{val_loss:.2f}
    save_top_k: 3
    save_last: true
    mode: min
    every_n_epochs: 1
  lr_monitor:
    _target_: pytorch_lightning.callbacks.LearningRateMonitor
    logging_interval: step
  swa:
    _target_: pytorch_lightning.callbacks.StochasticWeightAveraging
    swa_epoch_start: 0.8
    swa_lrs: 0.01
logger:
  _target_: pytorch_lightning.loggers.TensorBoardLogger
  save_dir: log
  name: lightning_logs
  version: test_code
trainer:
  max_epochs: 2
  accelerator: auto
  devices: 1
  log_every_n_steps: 10
  val_check_interval: 0.25
