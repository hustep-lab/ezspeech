task:
  _target_: ezspeech.task.asr.ASR_ctc_task
  model:
    architecture:
      _target_: ezspeech.models.conformer.Conformer
      d_input: 128
      d_dim: 512
      num_heads: 16
      num_layers: 20
      depthwise_conv_kernel_size: 31
      vocab_size: 98
    criterion:
      _target_: ezspeech.modules.criterion.CTCLoss
      reduction: mean
      zero_infinity: true
    manager:
      _target_: ezspeech.modules.optimizer.AdamOptimizer
      optimizer:
        lr: 0.01
        betas:
        - 0.9
        - 0.999
        eps: 1.0e-09
        weight_decay: 0.01
      scheduler:
        step_size: 30
        gamma: 0.1
  dataset:
    trainset:
      _target_: ezspeech.data.asr.ASRDataset
      filepath: /home4/khanhnd/vivos/train.tsv
      vocab_file: ezspeech/resources/vocab.txt
      feature_type: Mel
    valset:
      _target_: ezspeech.data.asr.ASRDataset
      filepath: /home4/khanhnd/vivos/test.tsv
      vocab_file: ezspeech/resources/vocab.txt
      feature_type: Mel
    batch_size: 4
callbacks:
  checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: train_loss_step
    filename: model-{epoch:02d}-{val_loss:.2f}
    save_top_k: 3
    save_last: true
    mode: min
    every_n_epochs: 1
  lr_monitor:
    _target_: pytorch_lightning.callbacks.LearningRateMonitor
    logging_interval: step
  swa:
    _target_: pytorch_lightning.callbacks.StochasticWeightAveraging
    swa_epoch_start: 0.8
    swa_lrs: 0.01
logger:
  _target_: pytorch_lightning.loggers.TensorBoardLogger
  save_dir: log
  name: lightning_logs
  version: test_code
trainer:
  max_epochs: 40
  accelerator: auto
  devices: 1
  log_every_n_steps: 10
